{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonzhou/opt/anaconda3/envs/dl_proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from utils.data import COCOAEDataset, collate_fn\n",
    "from utils.transforms import get_transform\n",
    "from utils.transforms import ResizeTransform\n",
    "from noise.scheduler import NoiseScheduler, LinearMaskScheduler, mask_image\n",
    "from models.masked_autoencoder import MaskedAEConfig, MaskedAutoEncoderForPretraining, MaskedAutoEncoderForCaptioning, MaskedAutoEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached annotations...\n"
     ]
    }
   ],
   "source": [
    "original_dataset = COCOAEDataset(root=\"coco/images/train2017/\",\n",
    "                        annFile=\"coco/annotations/ann2017/captions_train2017.json\",\n",
    "                        transform=get_transform(),\n",
    "                        tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased', cache_dir='cache/'),\n",
    "                        ignore_cache=False,\n",
    "                        train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(original_dataset, range(5))\n",
    "val_dataset = Subset(original_dataset, range(100, 110))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn(train_dataset.dataset.tokenizer.pad_token_id),\n",
    "                        pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn(val_dataset.dataset.tokenizer.pad_token_id),\n",
    "                        pin_memory=True)\n",
    "\n",
    "noise_scheduler = LinearMaskScheduler(vocab_size=len(train_dataset.dataset.tokenizer), masking_ratio=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MaskedAEConfig(len(train_dataset.dataset.tokenizer))\n",
    "pretrained = MaskedAutoEncoder(config).to(DEVICE)\n",
    "checkpoint = torch.load(\"checkpoints/base_0\",map_location=torch.device('cpu'))\n",
    "pretrained.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoEncoderForCaptioning(MaskedAEConfig(len(train_dataset.dataset.tokenizer)), pretrained=pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=4e-5)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1.5e-4, betas=(0.9, 0.95), weight_decay=0.03)\n",
    "\n",
    "image_loss = torch.nn.MSELoss()\n",
    "caption_loss = torch.nn.CrossEntropyLoss()\n",
    "# caption_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18]) torch.Size([4, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Caption Loss : 10.2:   0%|          | 0/10 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18, 30522])\n",
      "Original: [CLS] a man with a red helmet on a small moped on a dirt road. [SEP]\n",
      "Original: [CLS] a young boy barefoot holding an umbrella touching the horn of a cow [SEP] [PAD] [PAD] [PAD]\n",
      "Original: [CLS] he is listening intently to the computer at school. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Original: [CLS] a woman wearing a net on her head cutting a cake. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Generated: on on on on on on on on on on on on on on on on on on\n",
      "Generated: umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella\n",
      "Generated: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Generated: cake cake cake cake cake cake woman woman cake cake cake cake cake cake cake cake cake cake\n",
      "torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Caption Loss : 10.3:   0%|          | 0/10 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 30522])\n",
      "Original: [CLS] a boy wearing headphones using one computer in a long row of computers [SEP]\n",
      "Generated: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Caption Loss : 10.3:  10%|█         | 1/10 [00:14<02:10, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 21]) torch.Size([4, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Caption Loss : 10.2:  10%|█         | 1/10 [00:17<02:10, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 21, 30522])\n",
      "torch.Size([1, 12]) torch.Size([1, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Caption Loss : 10.3:  10%|█         | 1/10 [00:22<02:10, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Caption Loss : 10.3:  20%|██        | 2/10 [00:24<01:35, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18]) torch.Size([4, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Caption Loss : 10.2:  20%|██        | 2/10 [00:27<01:35, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18, 30522])\n",
      "torch.Size([1, 17]) torch.Size([1, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Caption Loss : 10.3:  20%|██        | 2/10 [00:32<01:35, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Caption Loss : 10.3:  30%|███       | 3/10 [00:34<01:16, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Caption Loss : 10.3:  30%|███       | 3/10 [00:36<01:16, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 30522])\n",
      "torch.Size([1, 17]) torch.Size([1, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Caption Loss : 10.3:  30%|███       | 3/10 [00:42<01:16, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Caption Loss : 10.3:  40%|████      | 4/10 [00:44<01:03, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 17]) torch.Size([4, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Caption Loss : 10.2:  40%|████      | 4/10 [00:47<01:03, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 17, 30522])\n",
      "torch.Size([1, 37]) torch.Size([1, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Caption Loss : 10.3:  40%|████      | 4/10 [00:52<01:03, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 37, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Caption Loss : 10.3:  50%|█████     | 5/10 [00:54<00:52, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Caption Loss : 10.2:  50%|█████     | 5/10 [00:57<00:52, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 30522])\n",
      "Original: [CLS] a child holding a flowered umbrella and petting a yak. [SEP]\n",
      "Original: [CLS] children sitting at computer stations on a long table. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Original: [CLS] a man riding on the back of a motorcycle. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Original: [CLS] a woman wearing a net on her head cutting a cake. [SEP] [PAD] [PAD]\n",
      "Generated: cow cow cow cow cow cow cow cow cow cow cow cow cow cow cow cow\n",
      "Generated: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] blast blast [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Generated: on a a on a on on a a a a on on on on on\n",
      "Generated: cake cake woman cake woman woman cake woman cake woman cake woman woman woman cake cake\n",
      "torch.Size([1, 14]) torch.Size([1, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Caption Loss : 10.3:  50%|█████     | 5/10 [01:02<00:52, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 30522])\n",
      "Original: [CLS] a little boy wearing headphones and looking at a computer monitor [SEP]\n",
      "Generated: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Caption Loss : 10.3:  60%|██████    | 6/10 [01:04<00:40, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 37]) torch.Size([4, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Caption Loss : 10.2:  60%|██████    | 6/10 [01:07<00:40, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 37, 30522])\n",
      "torch.Size([1, 15]) torch.Size([1, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Caption Loss : 10.3:  60%|██████    | 6/10 [01:13<00:40, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Caption Loss : 10.3:  70%|███████   | 7/10 [01:15<00:31, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18]) torch.Size([4, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Caption Loss : 10.2:  70%|███████   | 7/10 [01:17<00:31, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18, 30522])\n",
      "torch.Size([1, 13]) torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Caption Loss : 10.3:  70%|███████   | 7/10 [01:24<00:31, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Caption Loss : 10.3:  80%|████████  | 8/10 [01:25<00:21, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 37]) torch.Size([4, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Caption Loss : 10.2:  80%|████████  | 8/10 [01:28<00:21, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 37, 30522])\n",
      "torch.Size([1, 13]) torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Caption Loss : 10.3:  80%|████████  | 8/10 [01:34<00:21, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Caption Loss : 10.3:  90%|█████████ | 9/10 [01:36<00:10, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 21]) torch.Size([4, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Caption Loss : 10.2:  90%|█████████ | 9/10 [01:38<00:10, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 21, 30522])\n",
      "torch.Size([1, 15]) torch.Size([1, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Caption Loss : 10.3:  90%|█████████ | 9/10 [01:44<00:10, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Caption Loss : 10.3: 100%|██████████| 10/10 [01:46<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in (pbar := tqdm(range(10))):\n",
    "    for images, captions, lengths in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        captions = captions.to(DEVICE, non_blocking=True)\n",
    "        lengths = lengths.to(DEVICE, non_blocking=True)\n",
    "        # print(images.dtype, captions.dtype, lengths.dtype)\n",
    "\n",
    "        masked_images, text, targets, (image_positions, text_pad) = noise_scheduler.get_masked(images, captions, lengths, need_masks=True)\n",
    "        # print(masked_images.shape, masked_text.shape)   \n",
    "        print(captions.shape, text.shape)     \n",
    "        reconstructed_captions = model.forward(masked_images, captions, text_pad, image_positions)\n",
    "        print(reconstructed_captions.shape)\n",
    "        if epoch % 5 == 0:\n",
    "            # for c in text:\n",
    "            #     print(\"Original:\", train_dataset.dataset.tokenizer.decode(c))\n",
    "            for c in captions:\n",
    "                print(\"Original:\", train_dataset.dataset.tokenizer.decode(c))    \n",
    "            for c in reconstructed_captions:\n",
    "                print(\"Generated:\", train_dataset.dataset.tokenizer.decode(torch.argmax(c, dim=-1)))    \n",
    "            # for caption in reconstructed_captions:\n",
    "            #     # print(caption.shape)\n",
    "            #     values, indices = torch.topk(caption, 10)\n",
    "            #     # print(values, indices)\n",
    "            #     for i in indices:\n",
    "            #         print(train_dataset.dataset.tokenizer.decode(i), end = \", \")      \n",
    "            #     print(\"\")                               \n",
    "        # print(reconstructed_captions.shape, targets.shape)\n",
    "        shifted_original = captions[:,:-1]\n",
    "        shifted_reconstructed = reconstructed_captions[:,1:]\n",
    "        # print(shifted_original.shape, shifted_reconstructed.shape)\n",
    "        cap_loss = caption_loss(shifted_reconstructed.permute(0,2,1), shifted_original)\n",
    "        # print(cap_loss)\n",
    "        # cap_loss = caption_loss(reconstructed_captions, targets)\n",
    "        # cap_loss = caption_loss(reconstructed_captions.permute(0, 2, 1), padded_caption)\n",
    "        pbar.set_description(f\"Epoch: {epoch}, Caption Loss : {cap_loss.item():1.3}\")\n",
    "        cap_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a young boy barefoot holding an umbrella touching the horn of a cow [SEP]\n",
      "[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for text in captions:\n",
    "    print(train_dataset.dataset.tokenizer.decode(text))\n",
    "for caption in reconstructed_captions:\n",
    "    print(train_dataset.dataset.tokenizer.decode(torch.argmax(caption, dim=-1)))\n",
    "# for caption in reconstructed_captions:\n",
    "#     # print(caption.shape)\n",
    "#     values, indices = torch.topk(caption, 10)\n",
    "#     # print(values, indices)\n",
    "#     for i in indices:\n",
    "#         print(train_dataset.dataset.tokenizer.decode(i), end=\", \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, captions, lengths in val_dataloader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        captions = captions.to(DEVICE, non_blocking=True)\n",
    "        masked_images, masked_text, targets, (image_positions, rp) = noise_scheduler.get_masked(images, captions, lengths, need_masks=True)\n",
    "        print(targets)\n",
    "        for image in images:\n",
    "            plt.imshow(image.permute(1, 2, 0).detach().cpu().numpy())\n",
    "            plt.show() \n",
    "        reconstructed = model.forward(masked_images, lengths)\n",
    "        for caption in reconstructed:\n",
    "            values, indices = torch.topk(caption, 10)\n",
    "            for i in indices:\n",
    "                print(val_dataset.dataset.tokenizer.decode(i))\n",
    "        cap_loss = caption_loss(reconstructed, targets)\n",
    "        total_loss += cap_loss.item() * images.size(0)\n",
    "\n",
    "avg_loss = total_loss / len(val_dataset)\n",
    "print(f\"Validation Loss: {avg_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataLoader(train_dataset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn(train_dataset.dataset.tokenizer.pad_token_id),\n",
    "                        pin_memory=True)\n",
    "for images, captions, lengths in test:\n",
    "    optim.zero_grad()\n",
    "    images = images.to(DEVICE, non_blocking=True)\n",
    "    captions = captions.to(DEVICE, non_blocking=True)\n",
    "    lengths = lengths.to(DEVICE, non_blocking=True)\n",
    "    # print(images.dtype, captions.dtype, lengths.dtype)\n",
    "\n",
    "    masked_images, text, targets, (image_positions, text_pad) = noise_scheduler.get_masked(images, captions, lengths, need_masks=True)\n",
    "    print(text_pad)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
